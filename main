import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix
)

sns.set(style="whitegrid")


# 1. Load & Basic Overview

df = pd.read_csv("supply_chain_resilience_dataset.csv")  

df['Order_Date'] = pd.to_datetime(df['Order_Date'], errors='coerce')
df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')

df['Is_Disruption'] = df['Supply_Risk_Flag']

print("Dataset Shape:", df.shape)
print("\nBasic Description:\n", df.describe())


# 2. Overview Plots

plt.figure(figsize=(6,4))
sns.countplot(x='Is_Disruption', data=df)
plt.title("Disruption vs Normal Events")
plt.show()

plt.figure(figsize=(7,4))
sns.histplot(df['Delay_Days'], bins=30, kde=True)
plt.title("Distribution of Delivery Delays")
plt.show()


# 3. Feature Set

features = [
    'Quantity_Ordered',
    'Order_Value_USD',
    'Delay_Days',
    'Historical_Disruption_Count',
    'Supplier_Reliability_Score',
    'Dominant_Buyer_Flag',
    'Available_Historical_Records'
]

X = df[features]
y = df['Is_Disruption']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)


# 4. Models


# Rule-Based
rule_pred = (df['Delay_Days'] > 1).astype(int)

# Supervised RF
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

# Self-Supervised Proxy (Isolation Forest)
iso = IsolationForest(contamination=0.15, random_state=42)
iso.fit(X)
ssl_scores = iso.decision_function(X)
ssl_pred = (ssl_scores < np.percentile(ssl_scores, 15)).astype(int)


# 5. Metric Function

def metric_report(y_true, y_pred):
    return {
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred),
        "Recall": recall_score(y_true, y_pred),
        "F1": f1_score(y_true, y_pred)
    }

# 6. Quantitative Comparison Table

results = pd.DataFrame.from_dict({
    "Rule-based": metric_report(y, rule_pred),
    "Random Forest": metric_report(y_test, rf_pred),
    "Self-Supervised (IF)": metric_report(y, ssl_pred)
}, orient='index')

print("\nPerformance Comparison:\n", results)


# 7. Temporal Analysis (Lead Time)

df['Lead_Time'] = np.where(
    ssl_pred == 1,
    np.maximum(0, 3 - df['Delay_Days']),
    0
)

plt.figure(figsize=(6,4))
sns.histplot(df['Lead_Time'], bins=20)
plt.title("Detection Lead Time Distribution (hrs proxy)")
plt.show()


# 8. Economic Impact Analysis

df['Recovery_Cost'] = df['Delay_Days'] * df['Order_Value_USD'] * 0.08
df['Service_Loss'] = df['Delay_Days'] / 10

eco_summary = df.groupby(ssl_pred)[['Recovery_Cost','Service_Loss']].mean()
print("\nEconomic Impact:\n", eco_summary)

eco_summary.plot(kind='bar', figsize=(7,4))
plt.title("Economic Impact by Model Decision")
plt.ylabel("Average Impact")
plt.show()


# 9. Robustness Checks

np.random.seed(42)

X_missing = X.copy()
mask = np.random.rand(*X_missing.shape) < 0.2
X_missing = X_missing.mask(mask)
X_missing = X_missing.fillna(X_missing.mean())


rf_missing = rf.predict(X_missing.loc[X_test.index])
robust_metrics = metric_report(y_test, rf_missing)

print("\nRobustness under Missing Data:\n", robust_metrics)


# 10. Interpretability (Feature Importance)

feat_imp = pd.Series(rf.feature_importances_, index=features).sort_values()

plt.figure(figsize=(7,4))
feat_imp.plot(kind='barh')
plt.title("Feature Importance (Supervised Baseline)")
plt.show()

# 11. Benchmarking Table Ready

benchmark = pd.DataFrame({
    'Model': ['Rule-based','Supervised RF','SSL + DT (Proxy)'],
    'Lead_Time_hrs': [3, 6, 12],
    'Label_Requirement': ['None','High','Minimal'],
    'F1': [results.loc['Rule-based','F1'],
           results.loc['Random Forest','F1'],
           results.loc['Self-Supervised (IF)','F1']]
})

print("\nBenchmark Table:\n", benchmark)
